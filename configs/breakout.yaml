# Hyperparameters and configuration settings for Breakout environment

env:
  name: "ALE/Breakout-v5"
  grayscale: true 
  frame_stack: 4
  action_repeat: 4
  sticky_actions_p: 0.25
  clip_rewards: 'tanh'
  eval_noise: 0.0

# Replay buffer configuration
replay:
  capacity: 1_000_000 
  # The capacity of the replay buffer refers to the total number of individual time steps (or transitions) it can store. Each time the agent
  # takes a step in the environment, it generates a transition (observation, action, reward, done) which is then added to the buffer.  
  sequence_length: 50
  oversample_ends: True

model: 
  embed_dim: 1536
  recon_scale: 1
  pred_discount: True
  # reward_scale: 1.0
  discount_scale: 5.0
  
  rssm:
    category_size: 32
    class_size: 32
    deter_size: 600
    hidden_size: 600
    kl_balancing_alpha: 0.8
    kl_beta: 0.1
    kl_free: 0.0
    kl_loss_clip: 100.0
    min_std: 0.1
    
  actor_critic:
    hidden_size: 400
    entropy_coeff: 'linear(1e-2,3e-4,1e6)'  # Start with high entropy, decay faster

# Training configuration
training:
  total_env_frames: 100_000_000
  burn_in_frames: 50_000
  batch_size: 50
  world_model_update_interval: 16
  world_model_lr: 2e-4
  actor_lr: 4e-5
  critic_lr: 1e-4
  model_grad_clip: 100.0
  actor_grad_clip: 100.0
  value_grad_clip: 100.0
  imagination_horizon: 15
  lambda_return: 0.95
  discount: 0.999
  slow_value_target: True
  slow_target_update: 100
  slow_target_fraction: 1.0
  checkpoint_interval_steps: 1_000_000
  weight_decay: 1e-6

# Logging configuration
logging:
  wandb: true
  amp: true
  checkpoint_dir: "checkpoints-breakout"
  save_recon_images: True                                                                                                               â”‚
  recon_image_dir: "./recon_images" 
  save_recon_after_frames: 1_000_000 # New parameter: start saving recon images after 100,000 frames 
  stop_save_recon_after_frames: 1_001_000 # New parameter: stop saving recon images after 5,000,000 frames

seed: 42
