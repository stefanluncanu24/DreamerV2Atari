solar-aardvark-31:
    Reward Predictor deterministic
    Decoder Deterministic
    => change in loss function:

        - recon_dist = decoder(latent_states.view(-1, latent_states.shape[-1]))                                                                   │
 │ 139  - recon_loss = -recon_dist.log_prob(obs_batch.view(-1, *obs_shape) + 0.5).mean()                                                          │
 │ 138  + recon_pred = decoder(latent_states.view(-1, latent_states.shape[-1]))                                                                   │
 │ 139  + recon_loss = F.mse_loss(recon_pred, obs_batch.view(-1, *obs_shape) + 0.5)                                                               │
 │ 140                                                                                                                                            │
 │ 141  - reward_dist = reward_predictor(latent_states)                                                                                           │
 │ 142  - reward_loss = -reward_dist.log_prob(reward_batch).mean()                                                                                │
 │ 141  + reward_pred = reward_predictor(latent_states)                                                                                           │
 │ 142  + reward_loss = F.mse_loss(reward_pred, reward_batch)  

    bug changes:
    # Predict rewards, discounts, and values for the imagined trajectories                                                                  │
 │ 41   - imagined_rewards = reward_predictor(imagined_latent_states).mean                                                                        │                                                             │
 │ 41   + imagined_rewards = reward_predictor(imagined_latent_states)                                                                             │

olive-bee-24 (BASE):

# debug output predictions by decoder!
