creates Files in terminal : "New-Item -ItemType File requirements.txt"

makes directories: "mkdir configs, src, src/envs, src/models"

gradient clipping:
- Gradient clipping ensures the gradient vector g has a norm at most equal to the threshold. 
- Gradient clipping is a method where the error derivative is changed or clipped to a threshold during backward propagation through the network, and the clipped gradients are used to update the weights. 
- Gradient clipping is a solution to exploding gradient problem

batch_size:
- Number of samples used per training step.
- Larger batch sizes can stabilize training but require more memory.

instaling requirements: pip install -r requirements.txt

ELU: ELU is a activation function used in neural networks which is an advanced version of widely used relu activation function.Exponential Linear Unit (ELU) is an activation function that modifies the negative part of ReLU by applying an exponential curve. It allows small negative values instead of zero which improves learning dynamics.
https://www.geeksforgeeks.org/deep-learning/elu-activation-function-in-neural-network/

gemini CLI: npx https://github.com/google-gemini/gemini-cli

flatten: z_t = z_t.view(-1, self.stoch_size)

Softplus: SoftPlus is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive
