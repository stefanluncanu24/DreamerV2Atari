Yes, that's correct. The reason it saves 2500 images per call is due to how the data is processed in train.py.

Batching: The replay buffer samples a batch of data. According to your breakout.yaml configuration, this batch consists of:

batch_size: 50 sequences
sequence_length: 50 steps per sequence
Processing: In train.py, the code processes these sequences to generate latent states (latent_states). This results in a tensor with a shape of (batch_size, sequence_length, feature_size), which is (50, 50, feature_size).

Decoding: Before being passed to the decoder to reconstruct the images, this latent_states tensor is reshaped with .view(-1, ...). This flattens the first two dimensions (batch_size and sequence_length).

The shape becomes (50 * 50, feature_size), which is (2500, feature_size).
Saving: The output of the decoder, recon_pred, therefore has a shape where the first dimension is 2500. When this tensor is passed to save_reconstruction_predictions, the loop for i in range(recon_pred_tensor.shape[0]): iterates 2500 times, saving one image for each step in every sequence of the batch.